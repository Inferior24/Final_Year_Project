This project is a Retrieval-Augmented Generation (RAG) prototype built using FAISS as the vector database and a lightweight LLM (through Ollama / Hugging Face / LangChain).
The goal is to experiment with embeddings, retrieval, and response generation to understand how RAG pipelines actually work.

Rather than building the entire pipeline in one go, the focus (Sept 12â€“13) is on:

Setting up the environment

Generating embeddings from a sample dataset (e.g., text corpus, small JSON, or CSV)

Indexing embeddings in FAISS

Running a basic retrieval query

ğŸ› ï¸ Tech Stack

Python 3.10+

FAISS (Facebook AI Similarity Search) â€“ for vector indexing & retrieval

LangChain â€“ to connect embeddings, retrieval, and generation

LLM Provider:

Ollama
 (local) or

Hugging Face Transformers
 (cloud/local)

Database (optional, future): MongoDB for metadata storage

ğŸ“… Timeline
ğŸ”¹ Phase 1: Progress Days (Sept 12â€“13)

âœ… Install FAISS & verify installation

âœ… Create a Python script to generate embeddings for sample text data

âœ… Store embeddings in a FAISS index

âœ… Perform similarity search (e.g., top-3 most similar sentences)

âœ… Document errors, learnings, and fixes

ğŸ‘‰ AI tools to assist here:

Use ChatGPT for debugging code errors and designing function structure

Use GitHub Copilot / Cursor AI for quick function autocompletions

Use Claude / ChatGPT for summarizing concepts (FAISS indexing, embeddings)

ğŸ”¹ Phase 2: Understanding & Documentation (Sept 18â€“19)

Write detailed notes on:

How FAISS indexing works

How embeddings are generated

How retrieval integrates with generation

Document your workflow (with code snippets + diagrams)

ğŸ”¹ Phase 3: Presentation (Sept 20, Saturday)

Prepare slides (Google Slides / PPT)

Demo simple retrieval example

Explain concepts with visuals instead of heavy code
